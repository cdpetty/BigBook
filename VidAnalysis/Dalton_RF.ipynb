{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "# Edit path if need be (shouldn't need to b/c we all have the same folder structure)\n",
    "CSV_PATH = '../VidAnalysis/all_data'\n",
    "FILE_EXTENSION = '_all.csv'\n",
    "GENRES = ['country', 'edm', 'pop', 'rap', 'rock']\n",
    "\n",
    "# Containers for the data frames\n",
    "genre_dfs = {}\n",
    "all_genres = None\n",
    "\n",
    "\n",
    "# Read in the 5 genre's of CV's\n",
    "for genre in GENRES:\n",
    "    genre_csv_path = path.join(CSV_PATH, genre) + FILE_EXTENSION\n",
    "    genre_dfs[genre] = pd.read_csv(genre_csv_path)\n",
    "\n",
    "all_genres = pd.concat(genre_dfs.values())\n",
    "len(all_genres)\n",
    "\n",
    "# genre_dfs is now a dictionary that contains the 5 different data frames\n",
    "# all_genres is a dataframe that contains all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_new_headers(old_headers):\n",
    "    headers = ['colors_' + str(x+1) + '_' for x in range(10)]\n",
    "    h = []\n",
    "    for x in headers:\n",
    "        h.append(x + 'red')\n",
    "        h.append(x + 'blue')\n",
    "        h.append(x + 'green')\n",
    "    return old_headers + h + ['genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Genres\n",
    "Below, we make the genres ordinal to fit in the random forest classifiers. We add a new column to our dataframe to do so, write a function to populate it, and run it across the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genre_to_ordinal(genre_in):\n",
    "    if(genre_in == \"country\"):\n",
    "        return \"0\"\n",
    "    elif(genre_in == \"pop\"):\n",
    "        return \"1\"\n",
    "    elif(genre_in == \"rock\"):\n",
    "        return \"2\"\n",
    "    elif(genre_in == \"edm\"):\n",
    "        return \"3\"\n",
    "    elif(genre_in == \"rap\"):\n",
    "        return \"4\"\n",
    "    else:\n",
    "        return genre_in\n",
    "    \n",
    "all_genres['genre_ordinal'] = all_genres.genre.apply(genre_to_ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add in some boolean genre classifiers to make our analysis more fine-grained. Rather than saying \"we predict this video is country with 50% confidence\", we could say \"this video is not edm with 90% confidence\" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding is_country flag\n",
    "def is_country(genre_in):\n",
    "    if(genre_in == \"country\"):\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "    \n",
    "all_genres['is_country'] = all_genres.genre.apply(is_country)\n",
    "\n",
    "# Adding is_country flag\n",
    "def is_rock(genre_in):\n",
    "    if(genre_in == \"rock\"):\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "    \n",
    "all_genres['is_rock'] = all_genres.genre.apply(is_rock)\n",
    "\n",
    "# Adding is_edm flag\n",
    "def is_edm(genre_in):\n",
    "    if(genre_in == \"edm\"):\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "    \n",
    "all_genres['is_edm'] = all_genres.genre.apply(is_edm)\n",
    "\n",
    "# Adding is_rap flag\n",
    "def is_rap(genre_in):\n",
    "    if(genre_in == \"rap\"):\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "    \n",
    "all_genres['is_rap'] = all_genres.genre.apply(is_rap)\n",
    "\n",
    "# Adding is_country flag\n",
    "def is_pop(genre_in):\n",
    "    if(genre_in == \"pop\"):\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "    \n",
    "all_genres['is_pop'] = all_genres.genre.apply(is_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train Sets\n",
    "We create our training and test sets by splitting all_genres by genre, and making 10 of each genre train and 10 test. We aggregate by genre to make our full train and full test sets, each containing 50 records of various genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\t203\n",
      "Test:\t\t203\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Subset all_genres to group by individual genres\n",
    "country_records  = all_genres[all_genres[\"genre\"] == \"country\"]\n",
    "rock_records     = all_genres[all_genres[\"genre\"] == \"rock\"]\n",
    "pop_records      = all_genres[all_genres[\"genre\"] == \"pop\"]\n",
    "edm_records      = all_genres[all_genres[\"genre\"] == \"edm\"]\n",
    "rap_records      = all_genres[all_genres[\"genre\"] == \"rap\"]\n",
    "\n",
    "# From the subsets above, create train and test sets from each\n",
    "country_train = country_records.head(43)\n",
    "country_test  = country_records.tail(43)\n",
    "rock_train    = rock_records.head(35)\n",
    "rock_test     = rock_records.tail(35)\n",
    "pop_train     = pop_records.head(41)\n",
    "pop_test      = pop_records.tail(41)\n",
    "edm_train     = edm_records.head(40)\n",
    "edm_test      = edm_records.tail(40)\n",
    "rap_train     = rap_records.head(44)\n",
    "rap_test      = rap_records.tail(44)\n",
    "\n",
    "# Create big training and big test set for analysis\n",
    "training_set = pd.concat([country_train,rock_train,pop_train,edm_train,rap_train])\n",
    "test_set     = pd.concat([country_test,rock_test,pop_test,edm_test,rap_test])\n",
    "\n",
    "print \"Training:\\t\" , len(training_set)\n",
    "print \"Test:\\t\\t\" , len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generating Random Forests\n",
    "We start generating our random forests, and output a relative accuracy and a confusion matrix. In this first one, we simply factor in non-color variables (rating, likes, dislikes, length and viewcount), and run it across all records to predict an ordinal genre value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.566502463054\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2   3   4\n",
       "Actual                       \n",
       "0          32   0   0   7   4\n",
       "1           2   5  28   6   0\n",
       "2           5  15   5   6   4\n",
       "3           8   5   1  14  12\n",
       "4           2   7   2   7  26"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Predicting based solely on non-color features\n",
    "clf = RandomForestClassifier(n_estimators=11)\n",
    "meta_data_features = ['rating', 'likes','dislikes','length','viewcount']\n",
    "y, _ = pd.factorize(training_set['genre_ordinal'])\n",
    "clf = clf.fit(training_set[meta_data_features], y)\n",
    "\n",
    "z, _ = pd.factorize(test_set['genre_ordinal'])\n",
    "print clf.score(test_set[meta_data_features],z)\n",
    "pd.crosstab(test_set.genre_ordinal, clf.predict(test_set[meta_data_features]),rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, this method yields relatively poor results. This is because there's no distinct clusters being created by our random forest, and simple viewer statistics tell us nothing about what kind of video we're watching. However, we see that country, rap and pop are initially somewhat distinct (diagonal is the highest value), and rock and edm are getting mistaken for one another. Let's see if we can't make something of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Only Color\n",
    "Below, we do the same random forest as above, but going strictly off of average frame color for the video.\n",
    "\n",
    "We found the most commonly appearing color in each frame and called it the 'frame mode'. We then took all of the frame modes and found the 10 most common of them. Those became the 'color data' we use to analyze videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.246305418719\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1  2   3  4\n",
       "Actual                     \n",
       "0          19   6  3   6  9\n",
       "1           8  11  8   7  7\n",
       "2          16   7  3   7  2\n",
       "3          15   5  4   8  8\n",
       "4           8   6  9  13  8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=11)\n",
    "color_features = gen_new_headers([])[:-1]\n",
    "\n",
    "# Predicting based solely on colors\n",
    "y, _ = pd.factorize(training_set['genre_ordinal'])\n",
    "clf = clf.fit(training_set[color_features], y)\n",
    "\n",
    "z, _ = pd.factorize(test_set['genre_ordinal'])\n",
    "print clf.score(test_set[color_features],z)\n",
    "pd.crosstab(test_set.genre_ordinal, clf.predict(test_set[color_features]),rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually yields worse results than just the viewer statistics, because the color of a video by itself does not determine the genre. If rappers only had red in their videos and rockers only had black this might be somewhat accurate, but that's just not the case. But, what if we pair these findings with our initial viewer statistics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.467980295567\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0  1   2  3   4\n",
       "Actual                     \n",
       "0          29  0   0  6   8\n",
       "1           1  7  26  6   1\n",
       "2          12  7   9  4   3\n",
       "3          10  7   2  7  14\n",
       "4           7  4   1  6  26"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=11)\n",
    "all_features = meta_data_features + color_features\n",
    "\n",
    "# Predicting based on colors and non-color features\n",
    "y, _ = pd.factorize(training_set['genre_ordinal'])\n",
    "clf = clf.fit(training_set[all_features], y)\n",
    "\n",
    "z, _ = pd.factorize(test_set['genre_ordinal'])\n",
    "print clf.score(test_set[all_features],z)\n",
    "pd.crosstab(test_set.genre_ordinal, clf.predict(test_set[all_features]),rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singling Out Pop and Rap\n",
    "Scores are expectedly low. It seems as if we're trying to make the classifier do way too much work, and are giving it very mediocre data to go off of. Recall that we're actually trying to determine WHICH genre a video is by the above code, not whether or not a video is of ONE specific genre. This brings back the binary classifiers that we created above, let's put those to use to see if we can improve these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We try pop and rap first, since they seem to be the most distinct by what we've gathered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871921182266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1\n",
       "Actual            \n",
       "0          156   6\n",
       "1           20  21"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=11)\n",
    "all_features = meta_data_features + color_features\n",
    "\n",
    "# Predicting based on colors and non-color features\n",
    "y, _ = pd.factorize(training_set['is_pop'])\n",
    "clf = clf.fit(training_set[all_features], y)\n",
    "\n",
    "z, _ = pd.factorize(test_set['is_pop'])\n",
    "print clf.score(test_set[all_features],z)\n",
    "pd.crosstab(test_set.is_pop, clf.predict(test_set[all_features]),rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743842364532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1\n",
       "Actual            \n",
       "0          138  21\n",
       "1           31  13"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=11)\n",
    "all_features = meta_data_features + color_features\n",
    "\n",
    "# Predicting based on colors and non-color features\n",
    "y, _ = pd.factorize(training_set['is_rap'])\n",
    "clf = clf.fit(training_set[all_features], y)\n",
    "\n",
    "z, _ = pd.factorize(test_set['is_rap'])\n",
    "print clf.score(test_set[all_features],z)\n",
    "pd.crosstab(test_set.is_rap, clf.predict(test_set[all_features]),rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're seeing above is a confusion matrix that, based on our training data, predicts whether or not a video in the test set is a pop video or not. In the \"predicted\" row, 0 means it predicts it's not a pop video, and that the 1 is. Likewise with the actual, 0 shows that the video actually wasn't a pop video, and the 1 shows that it was.\n",
    "\n",
    "The confusion matrix above is our first effort at utilizing these binary classifiers. Most of our videos aren't pop videos (40 aren't, 10 are), and the model did a good job of picking out those that aren't pop. However, we could use some improvement in the realm of \"false negatives\", where the model classified a video as not pop when it actually was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We recreated the tests above with each genre, and the results are below:\n",
    "\n",
    "##### Ranking performance of boolean classifiers, train and test sets of 50, respectively.\n",
    "- 1 - is_pop (.84 avg) \n",
    "- 2 - is_rap (.82 avg, fewer true negatives)\n",
    "- 3 - is_rock (.78 avg, too many true negatives)\n",
    "- 4 - is_edm (.--, DO NOT USE. Rarely predicts a positive edm value)\n",
    "- 5 - is_country (.--, DO NOT USE. Way too many false positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do these tests 50 times for sake of average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0   1\n",
      "Actual            \n",
      "0          154   8\n",
      "1           16  25\n",
      "Average Score for 50 is_pop iterations: 0.870443349754\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=11)\n",
    "\n",
    "# Average score over many iterations calculation\n",
    "loop_indices = range(0,50)\n",
    "cumsum = 0\n",
    "\n",
    "for i in loop_indices:\n",
    "    y, _ = pd.factorize(training_set['is_pop'])\n",
    "    clf = clf.fit(training_set[all_features], y)\n",
    "\n",
    "    z, _ = pd.factorize(test_set['is_pop'])\n",
    "    #print clf.score(test_set[all_features],z)\n",
    "    cumsum = cumsum + clf.score(test_set[all_features],z)\n",
    "    #print pd.crosstab(test_set.is_pop, clf.predict(test_set[all_features]),rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "    \n",
    "print \"Average Score for\",len(loop_indices),\"is_pop iterations:\", cumsum/len(loop_indices)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for 50 is_rap iterations: 0.794975369458\n"
     ]
    }
   ],
   "source": [
    "# Average score over many iterations calculation\n",
    "loop_indices = range(0,50)\n",
    "cumsum = 0\n",
    "\n",
    "for i in loop_indices:\n",
    "    y, _ = pd.factorize(training_set['is_rap'])\n",
    "    clf = clf.fit(training_set[all_features], y)\n",
    "\n",
    "    z, _ = pd.factorize(test_set['is_rap'])\n",
    "    #print clf.score(test_set[all_features],z)\n",
    "    cumsum = cumsum + clf.score(test_set[all_features],z)\n",
    "    #print pd.crosstab(test_set.is_pop, clf.predict(test_set[all_features]),rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "    \n",
    "print \"Average Score for\",len(loop_indices),\"is_rap iterations:\", cumsum/len(loop_indices)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than hard-coding each time we wanted to run something for average, we wrote a function that does it for us. All we have to do is pass in the boolean classifier in quotes (\"is_rock\", etc.), and the number of iterations that we want. Results are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multi_RF_averages(is_genre,num_iterations):\n",
    "    clf = RandomForestClassifier(n_estimators=11)\n",
    "    loop_indices = range(0,num_iterations)\n",
    "    cumsum = 0\n",
    "\n",
    "    for i in loop_indices:\n",
    "        y, _ = pd.factorize(training_set[is_genre])\n",
    "        clf = clf.fit(training_set[all_features], y)\n",
    "\n",
    "        z, _ = pd.factorize(test_set[is_genre])\n",
    "        cumsum = cumsum + clf.score(test_set[all_features],z)\n",
    "    \n",
    "    print \"Average Score for\",len(loop_indices),is_genre,\"iterations:\", cumsum/len(loop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for 50 is_pop iterations: 0.872413793103\n",
      "Average Score for 50 is_rap iterations: 0.794679802956\n",
      "Average Score for 50 is_rock iterations: 0.828177339901\n",
      "Average Score for 50 is_edm iterations: 0.76315270936\n",
      "Average Score for 50 is_country iterations: 0.786206896552\n"
     ]
    }
   ],
   "source": [
    "multi_RF_averages(\"is_pop\",50)\n",
    "multi_RF_averages(\"is_rap\",50)\n",
    "multi_RF_averages(\"is_rock\",50)\n",
    "multi_RF_averages(\"is_edm\",50)\n",
    "multi_RF_averages(\"is_country\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the above test with all genres, and as shown in above analysis, our country and edm typically have very low accuracy. We've seen above that edm and rock videos are getting mixed up with one another, so we assume that something is characteristic of these 2 genres that's not of everything else. We take out the edm values from our training and test datasets, hoping to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for 50 is_pop iterations: 0.865398773006\n",
      "Average Score for 50 is_rap iterations: 0.810674846626\n",
      "Average Score for 50 is_rock iterations: 0.78282208589\n",
      "Average Score for 50 is_edm iterations: 1.0\n",
      "Average Score for 50 is_country iterations: 0.755950920245\n"
     ]
    }
   ],
   "source": [
    "# Removing EDM for better analysis - makes is_pop and is_rap much more accurate\n",
    "training_set = pd.concat([country_train,rock_train,pop_train,rap_train])\n",
    "test_set     = pd.concat([country_test,rock_test,pop_test,rap_test])\n",
    "\n",
    "multi_RF_averages(\"is_pop\",50)\n",
    "multi_RF_averages(\"is_rap\",50)\n",
    "multi_RF_averages(\"is_rock\",50)\n",
    "multi_RF_averages(\"is_edm\",50)\n",
    "multi_RF_averages(\"is_country\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, what does this tell us? Based on our training data, we have the best chance of accurately classifying something as pop or not pop (under these conditions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find out which 2 are the most distinct, so we can make build our model based on that classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for 50 is_rock iterations: 0.906285714286\n",
      "Average Score for 50 is_rap iterations: 0.6\n",
      "Average Score for 50 is_country iterations: 0.298604651163\n",
      "Average Score for 50 is_pop iterations: 0.445365853659\n",
      "Average Score for 50 is_edm iterations: 0.9115\n"
     ]
    }
   ],
   "source": [
    "training_set = pd.concat([country_train,rock_train,edm_train,rap_train,pop_train])\n",
    "\n",
    "test_set     = pd.concat([rock_test])\n",
    "multi_RF_averages(\"is_rock\",50)\n",
    "\n",
    "test_set     = pd.concat([rap_test])\n",
    "multi_RF_averages(\"is_rap\",50)\n",
    "\n",
    "test_set     = pd.concat([country_test])\n",
    "multi_RF_averages(\"is_country\",50)\n",
    "\n",
    "test_set     = pd.concat([pop_test])\n",
    "multi_RF_averages(\"is_pop\",50)\n",
    "\n",
    "test_set     = pd.concat([edm_test])\n",
    "multi_RF_averages(\"is_edm\",50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rock and EDM have suprisingly distinct classifiers. We should dive into the videos and see what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for 50 is_edm iterations: 0.5152\n",
      "Average Score for 50 is_rock iterations: 0.579466666667\n"
     ]
    }
   ],
   "source": [
    "test_set     = pd.concat([edm_test,rock_test])\n",
    "multi_RF_averages(\"is_edm\",50)\n",
    "multi_RF_averages(\"is_rock\",50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs489]",
   "language": "python",
   "name": "conda-env-cs489-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
